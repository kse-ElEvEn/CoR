


Recent advancements in generative large language models (LLMs) have led to significant progress. However, ongoing quality issues in generated text highlight the necessity for further investigation and refinement. Evaluating the quality of these texts, especially the open-ended outputs that vary in structure and lack standard formats, remains a complex challenge.

In this study, we present the "Chain of Rebuttal (CoR)" framework, designed to assess open-ended texts produced by LLMs. This framework replicates human evaluation processes through collaborative discussion methods. It primarily utilizes LLMs like GPT-4 to simulate the repeated rebuttals typical of human evaluators.

Our approach integrates self-reflection and Chains of Thought (CoT), deepening and broadening the evaluation process. Additionally, we implement a feedback mechanism that steers discussions toward consensus, resulting in detailed evaluation reports that encompass error identification, error categorization, and scoring.

Experimental results demonstrate that our framework surpasses existing methods for evaluating open-ended texts, achieving the highest correlation with human assessments. This outcome confirms the framework's efficacy and its advancement in evaluating text generated by LLMs.

<img width="660" alt="image" src="https://github.com/user-attachments/assets/8f5d3ed3-7634-43a2-9ce5-d691d8eab195">




Key Contributions:

We introduce the 'Chain of Rebuttal' (CoR), a multi-agent evaluation framework aimed at enhancing scoring accuracy by providing detailed diagnostic reports for LLM-generated text.

We propose a novel method to incorporate self-reflection and Chain of Thought (CoT) within our CoR framework. Additionally, we develop a new feedback mechanism after each discussion round to address discrepancies and facilitate consensus.

We conduct extensive experiments on four story text datasetsâ€”two in English and two in Chinese, including one derived from Alipay's business story text dataset. The experimental results highlight the framework's effectiveness and its strong alignment with human evaluations.


