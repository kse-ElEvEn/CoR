# Chain of Rebuttal(CoR): Resolving Open-Ended Text Evaluation Problem with Multi-Agent Discussion Framework
Chain of Rebuttal: Resolving Open-Ended Text Evaluation Problem with Multi-Agent Discussion Framework


Recent developments in generative large language models (LLMs) have achieved notable breakthroughs; however, the persistent quality issues in the generated text underscore a critical need for further research and improvement.
Evaluating the quality of text produced by these models, particularly their open-ended outputs that vary in form and lack a fixed format, remains a challenging task. 
In this work, we introduce the  "Chain of Rebuttal (CoR)" framework to evaluate open-ended texts generated by LLMs. This framework simulates human evaluative processes by incorporating collaborative discussion techniques. The agents within our framework are primarily implemented using LLMs such as GPT-4, which play a crucial role in simulating the repeated rebuttals characteristic of human evaluation.

Our method combines self-reflection and Chains of Thought (CoT), enhancing the depth and breadth of the evaluate process. We also incorporate a feedback mechanism to guide discussions towards consensus, producing comprehensive evaluation reports that include error localization, error types and scoring.
Experimental results indicate that our method outperforms existing methods for open-ended text evaluation, achieving the highest correlation with human evaluations. This confirms its effectiveness and advancement in evaluating open-ended texts generated by LLMs. 

<img width="660" alt="image" src="https://github.com/user-attachments/assets/8f5d3ed3-7634-43a2-9ce5-d691d8eab195">


Our main contributions in this paper are:

We introduce the 'Chain of Rebuttal' (CoR), as shown in Figure \ref{framework}, a multi-agent  evaluation framework designed to improve scoring reliability by offering precise diagnostic reports for text generated by LLMs. This framework, which simulates the human-like process of repeated rebuttals during text evaluation, aims to refine judgments through detailed scrutiny and iterative discussions.

We propose an innovative method to incorporate self-reflection and Chain of Thought (CoT) into our CoR framework. Furthermore, we introduce a novel feedback mechanism after each discussion round to address disagreements and help reach consensus.

We conduct comprehensive experiments on two English and two Chinese story text datasets,  including one derived from Alipay's business story text dataset. The results of our experiments demonstrate the effectiveness of our framework and its strong correlation with human evaluations. 
